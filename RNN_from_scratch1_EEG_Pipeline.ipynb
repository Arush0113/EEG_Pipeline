{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "RNN from scratch1  EEG Pipeline.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyMcPPEZNRIxp/RPbJewau0p",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Arush0113/EEG_Pipeline-IITR/blob/main/RNN_from_scratch1_EEG_Pipeline.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wMdjyVc4yFlU"
      },
      "source": [
        "import numpy as np\n",
        "# import tensorflow as tf"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lpjf79B7xOkT"
      },
      "source": [
        ""
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qYgLrUpvyTC0"
      },
      "source": [
        "# Initialization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZpImRl4Eyc5m"
      },
      "source": [
        "class RNN():\n",
        "    def __init__(self, word_dim, hidden_dim = 100, bptt_truncate = 4):\n",
        "        # assign instance variable\n",
        "        self.word_dim = word_dim\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.bptt_truncate = bptt_truncate\n",
        "        # random initiate the parameters\n",
        "        self.U = np.random.uniform(-np.sqrt(1./word_dim), np.sqrt(1./word_dim), (hidden_dim, word_dim))\n",
        "        self.V = np.random.uniform(-np.sqrt(1./hidden_dim), np.sqrt(1./hidden_dim), (word_dim, hidden_dim))\n",
        "        self.W = np.random.uniform(-np.sqrt(1./hidden_dim), np.sqrt(1./hidden_dim), (hidden_dim, hidden_dim))"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1Ms7qNN-dDSq"
      },
      "source": [
        "Softmax Function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jxtJPslYSd8d"
      },
      "source": [
        "def softmax(x):\n",
        "  exps = np.exp(x)\n",
        "  return exps / np.sum(exps);"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xkjAzDqpdIBV"
      },
      "source": [
        "def softmaxNorm(self, x):\n",
        "  p = np.exp(x - np.max(x))\n",
        "  return p / np.sum(p);"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_0cURRfTSc8V"
      },
      "source": [
        "# Forward Pass"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p_0H_QB2SUvb"
      },
      "source": [
        "def forward_propagation(self, x):\n",
        "    # total num of time steps, len of vector x\n",
        "    T = len(x)\n",
        "    # during forward propagation, save all hidden stages in s, S_t = U .dot x_t + W .dot s_{t-1}\n",
        "    # we also need the initial state of s, which is set to 0\n",
        "    # each time step is saved in one row in sï¼Œeach row in s is s[t] which corresponding to an rnn internal loop time\n",
        "    s = np.zeros((T+1, self.hidden_dim))\n",
        "    s[-1] = np.zeros(self.hidden_dim)\n",
        "    # output at each time step saved as o, save them for later use\n",
        "    o = np.zeros((T, self.word_dim))\n",
        "    for t in np.arange(T):\n",
        "        # we are indexing U by x[t]. it is the same as multiplying U with a one-hot vector\n",
        "        s[t] = np.tanh(self.U[:, x[t]] + self.W.dot(s[t-1]))\n",
        "        o[t] = softmax(self.V.dot(s[t]))\n",
        "    return [o, s]\n",
        "\n",
        "RNN.forward_propagation = forward_propagation"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jyh_MhZkdhAK"
      },
      "source": [
        "Predict Function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 229
        },
        "id": "ODxUp6LZdgZj",
        "outputId": "f0cdab97-2d69-44af-f8b9-9b863d362794"
      },
      "source": [
        "def predict(self, x):\n",
        "    o, s = self.forward_propagation(x)\n",
        "    return np.argmax(o, axis = 1)\n",
        "\n",
        "RNN.predict = predict\n",
        "\n",
        "np.random.seed(10)\n",
        "model = RNN(vocabulary_size)\n",
        "o, s = model.forward_propagation(X_train[10])\n",
        "print(o.shape)\n",
        "print(o)\n",
        "\n",
        "predictions = model.predict(X_train[10])\n",
        "print(predictions.shape)\n",
        "print(predictions) "
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-30-8b73638f4aea>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mseed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mRNN\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvocabulary_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mo\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward_propagation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mo\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'X_train' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i__lKDtxbkX3"
      },
      "source": [
        "Loss Function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 212
        },
        "id": "VsJAQyzHUbjY",
        "outputId": "a84641b4-aa55-4055-90bd-acdfea9c14df"
      },
      "source": [
        "vocabulary_size = 8000\n",
        "\n",
        "def calculate_total_loss(self, x, y):\n",
        "    L = 0\n",
        "    # for each sentence ...\n",
        "    for i in np.arange(len(y)):\n",
        "        o, s = self.forward_propagation(x[i])\n",
        "        # we only care about our prediction of the \"correct\" words\n",
        "        correct_word_predictions = o[np.arange(len(y[i])), y[i]]\n",
        "        # add to the loss based on how off we were\n",
        "        L += -1 * np.sum(np.log(correct_word_predictions))\n",
        "    return L\n",
        "\n",
        "def calculate_loss(self, x, y):\n",
        "    # divide the total loss by the number of training examples\n",
        "    N = np.sum((len(y_i) for y_i in y))\n",
        "    return self.calculate_total_loss(x, y)/N\n",
        "\n",
        "RNN.calculate_total_loss = calculate_total_loss\n",
        "RNN.calculate_loss = calculate_loss\n",
        "\n",
        "print(\"Expected Loss for random prediction: %f\" % np.log(vocabulary_size))\n",
        "print(\"Actual loss: %f\" % model.calculate_loss(X_train[:1000], y_train[:1000]))"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Expected Loss for random prediction: 8.987197\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-29-657ce7ef22db>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Expected Loss for random prediction: %f\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvocabulary_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Actual loss: %f\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcalculate_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m1000\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m1000\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hJjQxjCPb3vv"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}